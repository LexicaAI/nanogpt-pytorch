# NanoGPT

A lightweight implementation of the GPT (Generative Pre-trained Transformer) model with a clean, modular architecture.

## Features

- ğŸ§  Clean implementation of GPT-2 architecture
- ğŸš€ Training with FineWeb-Edu dataset
- ğŸ“Š Evaluation on HellaSwag benchmark
- ğŸ”„ Support for distributed training
- ğŸ’¾ Checkpointing and model loading
- ğŸ› ï¸ Flexible configuration options

## Installation

Clone this repository:

```bash
git clone https://github.com/LexicaAI/nanogpt-pytorch.git
cd nanogpt-pytorch
```

Install the dependencies:

```bash
pip install -r requirements.txt
```

## Usage

### Training

Train a GPT model from scratch:

```bash
python scripts/train.py --batch_size 32 --steps 10000
```

Fine-tune a pre-trained GPT-2 model:

```bash
python scripts/train.py --model_type gpt2 --batch_size 32 --steps 5000
```

### Evaluation

Evaluate a trained model on the HellaSwag benchmark:

```bash
python scripts/evaluate.py --task hellaswag
```

Measure perplexity on the validation set:

```bash
python scripts/evaluate.py --task perplexity --split val
```

### Text Generation

Generate text with a trained model:

```bash
python scripts/generate.py --prompt "Once upon a time" --max_tokens 100 --temperature 0.8
```

## Project Structure

The repository is organized as follows:

```
nanogpt/
â”œâ”€â”€ nanogpt/                      # Main package
â”‚   â”œâ”€â”€ model/                    # Model architectures
â”‚   â”‚   â”œâ”€â”€ attention.py          # Attention mechanisms
â”‚   â”‚   â”œâ”€â”€ mlp.py                # MLP module
â”‚   â”‚   â”œâ”€â”€ block.py              # Transformer block
â”‚   â”‚   â””â”€â”€ gpt.py                # GPT model
â”‚   â”œâ”€â”€ tokenizer/                # Tokenizer architectures
â”‚   â”‚   â”œâ”€â”€ tokenizer.py          # Tokenization utilities
â”‚   â”œâ”€â”€ data/                     # Data processing modules
â”‚   â”‚   â”œâ”€â”€ dataloader.py         # DataLoader implementation
â”‚   â”‚   â”œâ”€â”€ datasets/             # Dataset-specific processors
â”‚   â”‚   â”‚   â”œâ”€â”€ hellaswag.py      # HellaSwag dataset
â”‚   â”‚   â”‚   â””â”€â”€ fineweb.py        # FineWeb dataset
â”‚   â”œâ”€â”€ training/                 # Training utilities
â”‚   â”‚   â”œâ”€â”€ config.py             # GPT-2 specific configuration
â”‚   â”‚   â”œâ”€â”€ trainer.py            # Training loop implementation
â”‚   â”‚   â””â”€â”€ optimizer.py          # Optimization utilities
â”‚   â”œâ”€â”€ evaluation/               # Evaluation utilities
â”‚   â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics
â”‚   â”‚   â””â”€â”€ evaluator.py          # Evaluation pipeline
â”‚   â””â”€â”€ utils/                    # Utility functions
â”‚       â”œâ”€â”€ distributed.py        # Distributed training utilities
â”‚       â”œâ”€â”€ checkpoint.py         # Model checkpointing
â”‚       â””â”€â”€ logging.py            # Logging utilities
â””â”€â”€ scripts/                      # Executable scripts
    â”œâ”€â”€ train.py                  # Training script
    â”œâ”€â”€ evaluate.py               # Evaluation script
    â””â”€â”€ generate.py               # Text generation script
```

## Customization

The implementation is designed to be modular and easily customizable. You can modify any component to adapt it to your specific needs.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

This implementation is inspired by:
- [Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [OpenAI GPT-2](https://github.com/openai/gpt-2)

## References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830) 